{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3, 3)\n",
      "(3, 4)\n",
      "[[1 2 3 1 2 3 1]\n",
      " [4 5 6 4 5 6 1]\n",
      " [7 8 9 7 8 9 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "b = a\n",
    "c = np.array([[1,2,3,1],[4,5,6,1],[7,8,9,1]])\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)\n",
    "a = np.c_[a,c]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_flip  0\n",
      "u (90570,)\n",
      "90570\n",
      "9430\n",
      "(9430, 2607)\n",
      "(9430, 18)\n",
      "(9430, 2625)\n",
      "0.061477310367671414\n",
      "Outer iter 0 rank=1 loss=0.9111548176594546\n",
      "FM rank=1, validation RMSE=0.9583\n",
      "0.061477310367671414\n",
      "Outer iter 1 rank=2 loss=0.8988839479036235\n",
      "FM rank=2, validation RMSE=0.9561\n",
      "0.061422104449597\n",
      "Outer iter 2 rank=3 loss=0.8869929043347394\n",
      "FM rank=3, validation RMSE=0.9540\n",
      "0.061422104449597\n",
      "Outer iter 3 rank=4 loss=0.8826266853774938\n",
      "FM rank=4, validation RMSE=0.9519\n",
      "0.061422104449597\n",
      "Outer iter 4 rank=4 loss=0.8808645167079675\n",
      "FM rank=4, validation RMSE=0.9511\n",
      "0.061355857347907695\n",
      "Outer iter 5 rank=5 loss=0.871848339547968\n",
      "FM rank=5, validation RMSE=0.9502\n",
      "0.06134481616429281\n",
      "Outer iter 6 rank=6 loss=0.8691060460407343\n",
      "FM rank=6, validation RMSE=0.9503\n",
      "0.061355857347907695\n",
      "Outer iter 7 rank=7 loss=0.8641616660602218\n",
      "FM rank=7, validation RMSE=0.9498\n",
      "0.06136689853152258\n",
      "Outer iter 8 rank=8 loss=0.8593857136870191\n",
      "FM rank=8, validation RMSE=0.9488\n",
      "0.06137793971513746\n",
      "Outer iter 9 rank=9 loss=0.8545891788106499\n",
      "FM rank=9, validation RMSE=0.9480\n",
      "result: 0.05747613997879109\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "\"\"\"Convex factorization machines\n",
    "Implements the solver by: Mathieu Blondel, Akinori Fujino, Naonori Ueda.\n",
    "\"Convex factorization machines\". Proc. of ECML-PKDD 2015\n",
    "http://www.mblondel.org/publications/mblondel-ecmlpkdd2015.pdf\n",
    "\"\"\"\n",
    "\n",
    "# Author: Vlad Niculae <vlad@vene.ro>\n",
    "# License: Simplified BSD\n",
    "\n",
    "# TODOS:\n",
    "# * implement warm starts and regularization paths\n",
    "# * options to ignore the diagonal of Z / to constrain Z to be PSD\n",
    "# * implement fully corrective refitting\n",
    "# * diagonal refit every K iter (requires reasonable estimate of new eigval)\n",
    "# * implement projected gradient baseline for comparison\n",
    "\n",
    "import array\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import LinearOperator, eigsh\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model.cd_fast import enet_coordinate_descent\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import scipy.io as scio\n",
    "import math\n",
    "\n",
    "def _find_basis(X, residual, **kwargs):\n",
    "    def _create_mv(X, residual):\n",
    "        if sp.issparse(X):\n",
    "            def mv(p):\n",
    "                return X.T * (residual * (X * p))\n",
    "        else:\n",
    "            def mv(p):\n",
    "                return np.dot(X.T, residual * np.dot(X, p))\n",
    "        return mv\n",
    "\n",
    "    n_features = X.shape[1]\n",
    "    grad = LinearOperator((n_features, n_features),\n",
    "                          matvec=_create_mv(X, residual),\n",
    "                          dtype=X.dtype)\n",
    "    _, p = eigsh(grad, k=1, **kwargs)\n",
    "    return p.ravel()\n",
    "\n",
    "\n",
    "class ConvexFM(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, alpha=1., beta=1., fit_intercept=False,\n",
    "                 fit_linear=False, max_iter=200,\n",
    "                 max_iter_inner=100, max_rank=None, warm_start=True,\n",
    "                 tol=1e-3, refit_iter=1000, eigsh_kwargs={},\n",
    "                 verbose=True, random_state=0):\n",
    "        \"\"\"Factorization machine with nuclear norm regularization.\n",
    "        minimizes 0.5 ∑(y - (b + w'x + <Z, xx'>))² + .5 * α||w||² + β||Z||_*\n",
    "        Z is implicitly stored as an eigendecomposition Z = P'ΛP\n",
    "        Implements the greedy coordinate descent solver from:\n",
    "            Convex Factorization Machines.\n",
    "            Mathieu Blondel, Akinori Fujino, Naonori Ueda.\n",
    "            Proceedings of ECML-PKDD 2015\n",
    "            http://www.mblondel.org/publications/mblondel-ecmlpkdd2015.pdf\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : float\n",
    "            L2 regulariation for linear term\n",
    "        beta : float,\n",
    "            Nuclear (trace) norm regularization for quadratic term\n",
    "        fit_intercept : bool, default: False\n",
    "            Whether to fit an intercept (b). Only used if ``fit_linear=True``.\n",
    "        fit_linear : bool, default: False\n",
    "            Whether to fit the linear term (b + w'x).\n",
    "        max_iter : int,\n",
    "            Number of alternative steps in the outer loop.\n",
    "        max_iter_inner : int,\n",
    "            Number of iterations when solving for Z\n",
    "        max_rank : int,\n",
    "            Budget for the representation of Z. Default: n_features\n",
    "        warm_start : bool, default: False\n",
    "            Warm starts, not fully implemented yet.\n",
    "        tol : bool,\n",
    "            Tolerance for all subproblems.\n",
    "        refit_iter : int,\n",
    "            Number of iterations for diagonal refitting (Lasso)\n",
    "        eigsh_kwargs : dict,\n",
    "            Arguments to pass to the ARPACK eigenproblem solver. Defaults are\n",
    "            ``tol=tol`` and ``maxiter=5000``.\n",
    "        verbose : int,\n",
    "            Degree of verbosity.\n",
    "        random_state : int or np.random.RandomState,\n",
    "            Random number generator (used in diagonal refitting).\n",
    "        Attributes\n",
    "        ----------\n",
    "        ridge_ : sklearn.linear_model.Ridge instance,\n",
    "            Fitted regressor for the linear part\n",
    "        lams_ : list,\n",
    "            Fitted eigenvalues of Z\n",
    "        P_ : list,\n",
    "            Fitted eigenvectors of Z\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.fit_linear = fit_linear\n",
    "        self.max_iter = max_iter\n",
    "        self.max_iter_inner = max_iter_inner\n",
    "        self.max_rank = max_rank\n",
    "        self.warm_start = warm_start\n",
    "        self.tol = tol\n",
    "        self.refit_iter = refit_iter\n",
    "        self.eigsh_kwargs = eigsh_kwargs\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def predict_quadratic(self, X, P=None, lams=None):\n",
    "        \"\"\"Prediction from the quadratic term of the factorization machine.\n",
    "        Returns <Z, XX'>.\n",
    "        \"\"\"\n",
    "        if P is None:\n",
    "            P = self.P_\n",
    "            lams = self.lams_\n",
    "\n",
    "        if not len(lams):\n",
    "            return 0\n",
    "\n",
    "        K = polynomial_kernel(X, np.array(P), degree=2, gamma=1, coef0=0)\n",
    "        return np.dot(K, lams)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.fit_linear:\n",
    "            y_hat = self.ridge_.predict(X)\n",
    "        else:\n",
    "            y_hat = np.zeros(X.shape[0])\n",
    "        y_hat += self.predict_quadratic(X)\n",
    "        return y_hat\n",
    "\n",
    "    def update_Z(self, X, y, verbose=True, sample_weight=None):\n",
    "        \"\"\"Greedy CD solver for the quadratic term of a factorization machine.\n",
    "        Solves 0.5 ||y - <Z, XX'>||^2_2 + ||Z||_*\n",
    "        Z implicitly stored as P'ΛP\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        rng = check_random_state(self.random_state)\n",
    "        P = self.P_\n",
    "        lams = self.lams_\n",
    "        old_loss = np.inf\n",
    "        max_rank = self.max_rank\n",
    "        if max_rank is None:\n",
    "            max_rank = n_features\n",
    "\n",
    "        ##\n",
    "        #residual = self.predict_quadratic(X) - y  # could optimize\n",
    "        #loss = self._loss(residual, sample_weight=sample_weight)\n",
    "        #rms = np.sqrt(np.mean((residual) ** 2))\n",
    "        #print(\"rank={} loss={}, RMSE={}\".format(0, loss, rms))\n",
    "        ##\n",
    "\n",
    "        for _ in range(self.max_iter_inner):\n",
    "            if self.rank_ >= max_rank:\n",
    "                break\n",
    "            residual = self.predict_quadratic(X) - y  # could optimize\n",
    "            if sample_weight is not None:\n",
    "                residual *= sample_weight\n",
    "            p = _find_basis(X, residual, **self.eigsh_kwargs)\n",
    "            P.append(p)\n",
    "            lams.append(0.)\n",
    "\n",
    "            # refit\n",
    "            refit_target = y.copy()\n",
    "            K = polynomial_kernel(X, np.array(P), degree=2, gamma=1, coef0=0)\n",
    "            if sample_weight is not None:\n",
    "                refit_target *= np.sqrt(sample_weight)\n",
    "                K *= np.sqrt(sample_weight)[:, np.newaxis]\n",
    "            K = np.asfortranarray(K)\n",
    "            lams_init = np.array(lams, dtype=np.double)\n",
    "\n",
    "            # minimizes 0.5 * ||y - K * lams||_2^2 + beta * ||w||_1\n",
    "            lams, _, _, _ = enet_coordinate_descent(\n",
    "                lams_init, self.beta, 0, K, refit_target,\n",
    "                max_iter=self.refit_iter, tol=self.tol, rng=rng, random=0,\n",
    "                positive=0)\n",
    "            P = [p for p, lam in zip(P, lams) if np.abs(lam) > 0]\n",
    "            lams = [lam for lam in lams if np.abs(lam) > 0]\n",
    "            self.rank_ = len(lams)\n",
    "            self.quadratic_trace_ = np.sum(np.abs(lams))\n",
    "\n",
    "            predict_quadratic = self.predict_quadratic(X, P, lams)\n",
    "            residual = y - predict_quadratic  # y is already shifted\n",
    "            loss = self._loss(residual, sample_weight=sample_weight)\n",
    "\n",
    "            if verbose > 0:\n",
    "                rms = np.sqrt(np.mean((residual) ** 2))\n",
    "                print(\"rank={} loss={}, RMSE={}\".format(self.rank_, loss, rms))\n",
    "\n",
    "            if np.abs(old_loss - loss) < self.tol:\n",
    "                break\n",
    "\n",
    "            old_loss = loss\n",
    "        self.P_ = P\n",
    "        self.lams_ = lams\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if not self.warm_start or not hasattr(self, 'P'):\n",
    "            self.P_ = []\n",
    "            self.lams_ = []\n",
    "            self.rank_ = 0\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            assert len(sample_weight) == len(y)\n",
    "\n",
    "        # adjust eigsh defaults\n",
    "        if 'maxiter' not in self.eigsh_kwargs:\n",
    "            self.eigsh_kwargs['maxiter'] = 5000\n",
    "        if 'tol' not in self.eigsh_kwargs:\n",
    "            self.eigsh_kwargs['tol'] = self.tol\n",
    "\n",
    "\n",
    "        self.ridge_norm_sq_ = 0\n",
    "        self.quadratic_trace_ = 0\n",
    "\n",
    "        if self.fit_linear:\n",
    "            self.ridge_ = Ridge(alpha=0.5 * self.alpha,\n",
    "                                fit_intercept=self.fit_intercept)\n",
    "            old_loss = np.inf\n",
    "            quadratic_pred = 0\n",
    "\n",
    "            for i in range(self.max_iter):\n",
    "                # fit linear\n",
    "                self.ridge_.fit(X, y - quadratic_pred,\n",
    "                                sample_weight=sample_weight)\n",
    "                linear_pred = self.ridge_.predict(X)\n",
    "                self.ridge_norm_sq_ = np.sum(self.ridge_.coef_ ** 2)\n",
    "\n",
    "#                 print(self._loss(y - (linear_pred + quadratic_pred),\n",
    "#                                sample_weight=sample_weight))\n",
    "\n",
    "\n",
    "                # fit quadratic\n",
    "                self.update_Z(X, y - linear_pred, verbose=self.verbose - 1,\n",
    "                              sample_weight=sample_weight)\n",
    "                quadratic_pred = self.predict_quadratic(X)\n",
    "\n",
    "                loss = self._loss(y - (linear_pred + quadratic_pred),\n",
    "                                  sample_weight=sample_weight)\n",
    "                correct_num = 0\n",
    "                res_y = linear_pred + quadratic_pred\n",
    "                for te in range(0, len(res_y)):\n",
    "                    if (res_y[te] >= 0 and y[te] == 1) or (res_y[te] < 0 and y[te] == -1):    \n",
    "                        correct_num = correct_num+1\n",
    "                print(correct_num / len(res_y))\n",
    "#                 if self.verbose:\n",
    "                loss_print = (((y - (linear_pred + quadratic_pred)) ** 2).sum() / X.shape[0]) ** 0.5\n",
    "                print(\"Outer iter {} rank={} loss={}\".format(i, self.rank_, loss_print))\n",
    "                if np.abs(old_loss - loss) < self.tol:\n",
    "                    break\n",
    "                old_loss = loss\n",
    "                \n",
    "                y_val_pred = fm.predict(X_val)\n",
    "                print(\"FM rank={}, validation RMSE={:.4f}\".format(\n",
    "                    fm.rank_,\n",
    "                    np.sqrt(mean_squared_error(y_val, y_val_pred))))\n",
    "                \n",
    "        else:\n",
    "            self.update_Z(X, y, verbose=self.verbose - 1,\n",
    "                          sample_weight=sample_weight)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _loss(self, residual, sample_weight=None):\n",
    "        loss = residual ** 2\n",
    "        if sample_weight is not None:\n",
    "            loss *= sample_weight\n",
    "        loss = loss.sum() + self.alpha * self.ridge_norm_sq_\n",
    "        loss *= 0.5\n",
    "        loss += self.beta * self.quadratic_trace_\n",
    "        return loss\n",
    "\n",
    "\n",
    "def make_multinomial_fm_dataset(n_samples, n_features, rank=5, length=50,\n",
    "                                random_state=None):\n",
    "    # Inspired by `sklearn.datasets.make_multilabel_classification`\n",
    "    rng = check_random_state(random_state)\n",
    "\n",
    "    X_indices = array.array('i')\n",
    "    X_indptr = array.array('i', [0])\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # pick a non-zero document length by rejection sampling\n",
    "        n_words = 0\n",
    "        while n_words == 0:\n",
    "            n_words = rng.poisson(length)\n",
    "        # generate a document of length n_words\n",
    "        words = rng.randint(n_features, size=n_words)\n",
    "        X_indices.extend(words)\n",
    "        X_indptr.append(len(X_indices))\n",
    "\n",
    "    X_data = np.ones(len(X_indices), dtype=np.float64)\n",
    "    X = sp.csr_matrix((X_data, X_indices, X_indptr),\n",
    "                      shape=(n_samples, n_features))\n",
    "    X.sum_duplicates()\n",
    "\n",
    "    true_w = rng.randn(n_features)\n",
    "    true_eigv = rng.randn(rank)\n",
    "    true_P = rng.randn(rank, n_features)\n",
    "\n",
    "    y = safe_sparse_dot(X, true_w)\n",
    "    y += ConvexFM().predict_quadratic(X, true_P, true_eigv)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    data = scio.loadmat('movielens/training_data_100k.mat')\n",
    "    testdata = scio.loadmat('movielens/test_data_100k.mat')\n",
    "#     data = scio.loadmat('movielens/training_data_10m_sparse.mat')\n",
    "#     testdata = scio.loadmat('movielens/test_data_10m_sparse.mat')\n",
    "#     data = scio.loadmat('amazon/training_data_video.mat')\n",
    "#     testdata = scio.loadmat('amazon/test_data_video.mat')\n",
    "#     data = scio.loadmat('amazon/training_data_patio.mat')\n",
    "#     testdata = scio.loadmat('amazon/test_data_patio.mat')\n",
    "#     data = scio.loadmat('amazon/training_data_music.mat') #1 10000\n",
    "#     testdata = scio.loadmat('amazon/test_data_music.mat')\n",
    "#     data = scio.loadmat('phishing/train_data')\n",
    "#     testdata = scio.loadmat('phishing/test_data')\n",
    "#     data = scio.loadmat('HetRec2011/train_data')\n",
    "#     testdata = scio.loadmat('HetRec2011/test_data')\n",
    "    X = data['train_X']\n",
    "    y = data['train_Y']\n",
    "    y = y.reshape((y.shape[0],))\n",
    "    \n",
    "    \n",
    "    noise_ratio = 0.1;\n",
    "    num_sample = y.shape[0];\n",
    "    num_flip = math.ceil(num_sample*noise_ratio);\n",
    "    print('number_flip ', num_flip);\n",
    "    re_idx = np.random.permutation(range(num_sample))\n",
    "    y[re_idx[1:num_flip]] = -y[re_idx[1:num_flip]];\n",
    "    \n",
    "    print('u',y.shape)\n",
    "    print(X.shape[0])\n",
    "    X_val = testdata['test_X']\n",
    "    y_val = testdata['test_Y']\n",
    "    y_val = y_val.reshape((y_val.shape[0],))\n",
    "    print(y_val.shape[0])\n",
    "    col = X.shape[1] - X_val.shape[1]\n",
    "    X_temp = np.zeros([X_val.shape[0],col])\n",
    "    print(X_val.shape)\n",
    "    print(X_temp.shape)\n",
    "#     np.c_[X_val,X_temp]\n",
    "    X_val = sp.hstack((X_val,X_temp)).toarray()\n",
    "    print(X_val.shape)\n",
    "#     n_samples, n_features = 1000, 3826\n",
    "#     rank = 200\n",
    "#     length = 5\n",
    "#     X, y = make_multinomial_fm_dataset(n_samples, n_features, rank, length,\n",
    "#                                        random_state=0)\n",
    "#     X, X_val, y, y_val = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "#     y += 0.01 * np.random.RandomState(0).randn(*y.shape)\n",
    "#     print(type(X))\n",
    "#     print(type(y))\n",
    "\n",
    "\n",
    "\n",
    "    # try ridge\n",
    "#     from sklearn.linear_model import RidgeCV\n",
    "#     ridge = RidgeCV(alphas=np.logspace(-4, 4, num=9, base=10),\n",
    "#                     fit_intercept=False)\n",
    "#     ridge.fit(X, y);\n",
    "#     y_val_pred = ridge.predict(X_val)\n",
    "#     print('RidgeCV validation RMSE={}'.format(\n",
    "#         np.sqrt(mean_squared_error(y_val, y_val_pred))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # convex factorization machine path\n",
    "    fm = ConvexFM(fit_linear=True, warm_start=False, max_iter=10, tol=1e-4,\n",
    "                  max_iter_inner=1, fit_intercept=True,\n",
    "                  eigsh_kwargs={'tol': 1}\n",
    "                  )\n",
    "    import scipy.sparse as sp\n",
    "#     0.000000001\n",
    "    fm.set_params(alpha=10, beta=10, fit_linear=True)\n",
    "    fm.fit(sp.vstack([X, X]), np.concatenate([y, y]))\n",
    "    y_val_pred = fm.predict(X_val)\n",
    "#     print(\"FM rank={}, validation RMSE={:.4f}\".format(\n",
    "#         fm.rank_,\n",
    "#         np.sqrt(mean_squared_error(y_val, y_val_pred))))\n",
    "    correct_num = 0\n",
    "    for i in range(0, len(y_val_pred)):\n",
    "        if (y_val_pred[i] >= 0 and y_val[i] == 1) or (y_val_pred[i] < 0 and y_val[i] == -1):    \n",
    "            correct_num = correct_num+1\n",
    "    print(\"result:\", correct_num / len(y_val_pred))\n",
    "#     for alpha in (0.01, 0.1, 1, 10):\n",
    "#         for beta in (10000, 500, 150, 100, 50, 1, 0.001):\n",
    "#             fm.set_params(alpha=alpha, beta=beta, fit_linear=True)\n",
    "    \n",
    "\n",
    "#             fm.fit(sp.vstack([X, X]), np.concatenate([y, y]))\n",
    "#             y_val_pred = fm.predict(X_val)\n",
    "#             print(\"FM rank={}, validation RMSE={:.2f}\".format(\n",
    "#                 fm.rank_,\n",
    "#                 np.sqrt(mean_squared_error(y_val, y_val_pred))))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     if True:\n",
    "#         for alpha in (0.01, 0.1, 1, 10):\n",
    "#             for beta in (10000, 500, 150, 100, 50, 1, 0.001):\n",
    "#                 fm.set_params(alpha=alpha, beta=beta)\n",
    "#                 fm.fit(X, y)\n",
    "#                 y_val_pred = fm.predict(X_val)\n",
    "#                 print(\"α={} β={}, rank={}, validation RMSE={:.2f}\".format(\n",
    "#                     alpha,\n",
    "#                     beta,\n",
    "#                     fm.rank_,\n",
    "#                     np.sqrt(mean_squared_error(y_val, y_val_pred))))\n",
    "\n",
    "#     if False:\n",
    "#         fm.set_params(alpha=0.1, beta=1, fit_linear=True)\n",
    "\n",
    "#         import scipy.sparse as sp\n",
    "\n",
    "#         fm.fit(sp.vstack([X, X]), np.concatenate([y, y]))\n",
    "#         y_val_pred = fm.predict(X_val)\n",
    "#         print(\"FM rank={}, validation RMSE={:.2f}\".format(\n",
    "#             fm.rank_,\n",
    "#             np.sqrt(mean_squared_error(y_val, y_val_pred))))\n",
    "\n",
    "#         fm.set_params(beta=1.)\n",
    "#         fm.fit(X, y, sample_weight=2 * np.ones_like(y))\n",
    "#         y_val_pred = fm.predict(X_val)\n",
    "#         print(\"FM rank={}, validation RMSE={:.2f}\".format(\n",
    "#             fm.rank_,\n",
    "#             np.sqrt(mean_squared_error(y_val, y_val_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outer iter 0 rank=1 loss=0.9085855683325472\n",
    "Outer iter 1 rank=2 loss=0.8901145317093029\n",
    "Outer iter 2 rank=3 loss=0.8847465750154758\n",
    "Outer iter 3 rank=4 loss=0.8824124578102228\n",
    "Outer iter 4 rank=5 loss=0.8806202524506291\n",
    "Outer iter 5 rank=5 loss=0.879160180777681\n",
    "Outer iter 6 rank=6 loss=0.8698841150097177\n",
    "Outer iter 7 rank=7 loss=0.8633370934767354\n",
    "Outer iter 8 rank=8 loss=0.857160156003752\n",
    "Outer iter 9 rank=9 loss=0.8522932921710188\n",
    "FM rank=9, validation RMSE=0.952\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Outer iter 0 rank=1 loss=0.7911070545731165\n",
    "FM rank=1, validation RMSE=0.8118\n",
    "0.025103923307780517\n",
    "Outer iter 1 rank=2 loss=0.784115604651498\n",
    "FM rank=2, validation RMSE=0.8059\n",
    "0.025103923307780517\n",
    "Outer iter 2 rank=3 loss=0.7773690722206189\n",
    "FM rank=3, validation RMSE=0.8001\n",
    "0.025103923307780517\n",
    "Outer iter 3 rank=4 loss=0.7722430377601218\n",
    "FM rank=4, validation RMSE=0.7960\n",
    "0.02510262467225686\n",
    "Outer iter 4 rank=5 loss=0.7662996279086021\n",
    "FM rank=5, validation RMSE=0.7923\n",
    "0.025101326036733203\n",
    "Outer iter 5 rank=6 loss=0.7604704709408506\n",
    "FM rank=6, validation RMSE=0.7881\n",
    "0.02510002740120955\n",
    "Outer iter 6 rank=7 loss=0.7540458149791232\n",
    "FM rank=7, validation RMSE=0.7843\n",
    "0.025101326036733203\n",
    "Outer iter 7 rank=8 loss=0.747855417613073\n",
    "FM rank=8, validation RMSE=0.7809\n",
    "0.025101326036733203\n",
    "Outer iter 8 rank=9 loss=0.7438191430134474\n",
    "FM rank=9, validation RMSE=0.7788\n",
    "0.02509483285911493\n",
    "Outer iter 9 rank=10 loss=0.7375473091363426\n",
    "FM rank=10, validation RMSE=0.7768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 4]\n"
     ]
    }
   ],
   "source": [
    "te = np.random.permutation(range(10))\n",
    "print(te[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
